{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import partial as partial_func\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\CSED\\\\semester-6\\\\pattern-recognetion\\\\labs\\\\lab2-clustering\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "  \"\"\"\n",
    "    Return list of all 125 rows (125 * 45)\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "      row = [float(value) for value in line.strip().split(',')] # Converting to float\n",
    "      data.append(row)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_2_generator(list_of_rows):\n",
    "    return [item for row in list_of_rows for item in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_1_generator(list_of_rows):\n",
    "    n = len(list_of_rows)\n",
    "    mean_sample = [0 for _ in range(len(list_of_rows[0]))]\n",
    "    for row in list_of_rows:\n",
    "        for i in range(len(row)):\n",
    "            mean_sample[i] += row[i]\n",
    "    return [x / n for x in mean_sample]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(approach): \n",
    "    \"\"\"\n",
    "        This function generates and splits tainging and tesing data condering the approach desired. \n",
    "        approach = 1 -> Taking the mean of each column in each segment resulting in 45 features for each data point.\n",
    "        approach = 2 -> Flattening all the features together in 45 x 125 = 5625 features for each data point.\n",
    "    \"\"\"\n",
    "    training_data, training_labels, testing_data, testing_labels = [], [], [], []\n",
    "\n",
    "    for activity in sorted(os.listdir(path)):\n",
    "        label = int(activity[1:]) - 1   # To make it zero-based\n",
    "        subjects_path = os.path.join(path, activity)\n",
    "        # subject_path = path + '/' + activity\n",
    "        for subject in sorted(os.listdir(subjects_path)):\n",
    "            segments_path = os.path.join(subjects_path, subject)\n",
    "            \n",
    "            for segment in sorted(os.listdir(segments_path)):\n",
    "                file_name =  os.path.join(segments_path, segment)\n",
    "                data_sample = []\n",
    "                \n",
    "                if approach == 1:\n",
    "                    data_sample = approach_1_generator(read_file(file_name))\n",
    "                elif approach == 2:\n",
    "                    data_sample = approach_2_generator(read_file(file_name))\n",
    "    \n",
    "                if int(segment[1:3]) <= 48: # Belongs to training data\n",
    "                    training_data.append(data_sample)\n",
    "                    training_labels.append(label)\n",
    "                else:\n",
    "                    testing_data.append(data_sample)\n",
    "                    testing_labels.append(label)\n",
    "    return  training_data , training_labels , testing_data , testing_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generated by taking the mean of each column in each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1 , training_labels_1 , testing_data_1 , testing_labels_1 = generate_data(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generated by flattening all the features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_2 , training_labels_2 , testing_data_2 , testing_labels_2 = generate_data(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_1, scaler_2 = StandardScaler(), StandardScaler()\n",
    "# Normalizing approach 1\n",
    "scaler_1.fit(training_data_1)\n",
    "normalized_training_data_1 = scaler_1.transform(training_data_1)\n",
    "normalized_testing_data_1 = scaler_1.transform(testing_data_1)\n",
    "\n",
    "# Normalizing approach 2\n",
    "scaler_2.fit(training_data_2)\n",
    "normalized_training_data_2 = scaler_2.transform(training_data_2)\n",
    "normalized_testing_data_2 = scaler_2.transform(testing_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying dimensionality reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(normalized_training_data_2)\n",
    "reduced_training_data_2 = pca.transform(normalized_training_data_2)\n",
    "reduced_testing_data_2 = pca.transform(normalized_testing_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced training data shape:  (7296, 870)\n"
     ]
    }
   ],
   "source": [
    "print(\"Reduced training data shape: \", reduced_training_data_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(data, k, max_iter=1000, tol=1e-6):\n",
    "    # Initialize centroids randomly\n",
    "    centroids = data[np.random.choice(range(len(data)), k, replace=False)]\n",
    "    \n",
    "    # Initialize cluster assignments\n",
    "    clusters = np.zeros(len(data))\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        # Assign each data point to the nearest centroid\n",
    "        for i, point in enumerate(data):\n",
    "            distances = np.linalg.norm(point - centroids, axis=1)\n",
    "            clusters[i] = np.argmin(distances)\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids = np.array([np.mean(data[clusters == i], axis=0) for i in range(k)])\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.linalg.norm(new_centroids - centroids) < tol:\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "    \n",
    "    return clusters, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 1. 1. 2. 2. 2.]\n",
      "[[ 2.          1.33333333]\n",
      " [ 7.          5.        ]\n",
      " [11.          2.        ]]\n"
     ]
    }
   ],
   "source": [
    "# simple example usage for the kmeans function\n",
    "data = np.array([[1, 1], [2, 1], [3, 2], [7, 5], [6, 4], [8, 6], [10, 1], [11, 3], [12, 2]])\n",
    "# expected results\n",
    "# clusters = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "# centroids = [[1.5, 2.5], [3.5, 4.5], [5.5, 6.5]]\n",
    "clusters, centroids = kmeans(data, 3, tol=1e-6)\n",
    "print(clusters)\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, centroids):\n",
    "    clusters = np.zeros(len(data))\n",
    "    for i, point in enumerate(data):\n",
    "        distances = np.linalg.norm(point - centroids, axis=1)\n",
    "        clusters[i] = np.argmin(distances)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_mat(clusters, labels, k, n):\n",
    "    confusion_matrix = np.zeros((k, n))\n",
    "    for i in range(len(labels)):\n",
    "        confusion_matrix[int(clusters[i])][labels[i]] += 1\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assign_clusters_labels(clusters, labels, k, n):\n",
    "    confusion_matrix = get_confusion_mat(clusters, labels, k, n)\n",
    "    return np.argmax(confusion_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(clusters, labels, cluster_assignments):\n",
    "    correct = 0\n",
    "    for i in range(len(clusters)):\n",
    "        if cluster_assignments[int(clusters[i])] == labels[i]:\n",
    "            correct += 1\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_separator():\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans for approach 1 with K = 8\n",
      "Training accuracy:  0.3404605263157895\n",
      "Testing accuracy:  0.3393640350877193\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "KMeans for approach 1 with K = 13\n",
      "Training accuracy:  0.47423245614035087\n",
      "Testing accuracy:  0.4758771929824561\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "KMeans for approach 1 with K = 19\n",
      "Training accuracy:  0.5677083333333334\n",
      "Testing accuracy:  0.5581140350877193\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "KMeans for approach 1 with K = 28\n",
      "Training accuracy:  0.6136239035087719\n",
      "Testing accuracy:  0.6112938596491229\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "KMeans for approach 1 with K = 38\n",
      "Training accuracy:  0.696546052631579\n",
      "Testing accuracy:  0.6979166666666666\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "KMeans for approach 2 with K = 8\n",
      "Training accuracy:  0.3145559210526316\n",
      "Testing accuracy:  0.31414473684210525\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "KMeans for approach 2 with K = 13\n",
      "Training accuracy:  0.4128289473684211\n",
      "Testing accuracy:  0.4166666666666667\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "KMeans for approach 2 with K = 19\n",
      "Training accuracy:  0.5529057017543859\n",
      "Testing accuracy:  0.5476973684210527\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "KMeans for approach 2 with K = 28\n",
      "Training accuracy:  0.6314418859649122\n",
      "Testing accuracy:  0.6430921052631579\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "KMeans for approach 2 with K = 38\n",
      "Training accuracy:  0.6137609649122807\n",
      "Testing accuracy:  0.6118421052631579\n",
      "--------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# KMeans for approach 1\n",
    "Ks = [8, 13, 19, 28, 38]\n",
    "for k in Ks:\n",
    "    print(\"KMeans for approach 1 with K =\", k)\n",
    "    # Training\n",
    "    training_predictions, training_centroids = kmeans(normalized_training_data_1, k)\n",
    "\n",
    "    # Testing\n",
    "    testing_predictions = predict(normalized_testing_data_1, training_centroids)\n",
    "\n",
    "    # Assigning labels\n",
    "    cluster_assignments = assign_clusters_labels(training_predictions, training_labels_1, k, 19)\n",
    "\n",
    "    # Calculating accuracy\n",
    "    training_accuracy = get_accuracy(training_predictions, training_labels_1, cluster_assignments)\n",
    "    testing_accuracy = get_accuracy(testing_predictions, testing_labels_1, cluster_assignments)\n",
    "\n",
    "    print(\"Training accuracy: \", training_accuracy)\n",
    "    print(\"Testing accuracy: \", testing_accuracy)\n",
    "    print_separator()\n",
    "\n",
    "# KMeans for approach 2\n",
    "for k in Ks:\n",
    "    print(\"KMeans for approach 2 with K =\", k)\n",
    "    # Training\n",
    "    training_predictions, training_centroids = kmeans(reduced_training_data_2, k)\n",
    "\n",
    "    # Testing\n",
    "    testing_predictions = predict(reduced_testing_data_2, training_centroids)\n",
    "\n",
    "    # Assigning labels\n",
    "    cluster_assignments = assign_clusters_labels(training_predictions, training_labels_2, k, 19)\n",
    "\n",
    "    # Calculating accuracy\n",
    "    training_accuracy = get_accuracy(training_predictions, training_labels_2, cluster_assignments)\n",
    "    testing_accuracy = get_accuracy(testing_predictions, testing_labels_2, cluster_assignments)\n",
    "\n",
    "    print(\"Training accuracy: \", training_accuracy)\n",
    "    print(\"Testing accuracy: \", testing_accuracy)\n",
    "    print_separator()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
