{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import partial as partial_func\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\CSED\\\\semester-6\\\\pattern-recognetion\\\\labs\\\\lab2-clustering\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "  \"\"\"\n",
    "    Return list of all 125 rows (125 * 45)\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "      row = [float(value) for value in line.strip().split(',')] # Converting to float\n",
    "      data.append(row)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_2_generator(list_of_rows):\n",
    "    return [item for row in list_of_rows for item in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_1_generator(list_of_rows):\n",
    "    n = len(list_of_rows)\n",
    "    mean_sample = [0 for _ in range(len(list_of_rows[0]))]\n",
    "    for row in list_of_rows:\n",
    "        for i in range(len(row)):\n",
    "            mean_sample[i] += row[i]\n",
    "    return [x / n for x in mean_sample]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(approach): \n",
    "    \"\"\"\n",
    "        This function generates and splits tainging and tesing data condering the approach desired. \n",
    "        approach = 1 -> Taking the mean of each column in each segment resulting in 45 features for each data point.\n",
    "        approach = 2 -> Flattening all the features together in 45 x 125 = 5625 features for each data point.\n",
    "    \"\"\"\n",
    "    training_data, training_labels, testing_data, testing_labels = [], [], [], []\n",
    "\n",
    "    for activity in sorted(os.listdir(path)):\n",
    "        label = int(activity[1:]) - 1   # To make it zero-based\n",
    "        subjects_path = os.path.join(path, activity)\n",
    "        # subject_path = path + '/' + activity\n",
    "        for subject in sorted(os.listdir(subjects_path)):\n",
    "            segments_path = os.path.join(subjects_path, subject)\n",
    "            \n",
    "            for segment in sorted(os.listdir(segments_path)):\n",
    "                file_name =  os.path.join(segments_path, segment)\n",
    "                data_sample = []\n",
    "                \n",
    "                if approach == 1:\n",
    "                    data_sample = approach_1_generator(read_file(file_name))\n",
    "                elif approach == 2:\n",
    "                    data_sample = approach_2_generator(read_file(file_name))\n",
    "    \n",
    "                if int(segment[1:3]) <= 48: # Belongs to training data\n",
    "                    training_data.append(data_sample)\n",
    "                    training_labels.append(label)\n",
    "                else:\n",
    "                    testing_data.append(data_sample)\n",
    "                    testing_labels.append(label)\n",
    "    return  training_data , training_labels , testing_data , testing_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generated by taking the mean of each column in each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1 , training_labels_1 , testing_data_1 , testing_labels_1 = generate_data(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generated by flattening all the features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_2 , training_labels_2 , testing_data_2 , testing_labels_2 = generate_data(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_1, scaler_2 = StandardScaler(), StandardScaler()\n",
    "# Normalizing approach 1\n",
    "scaler_1.fit(training_data_1)\n",
    "normalized_training_data_1 = scaler_1.transform(training_data_1)\n",
    "normalized_testing_data_1 = scaler_1.transform(testing_data_1)\n",
    "\n",
    "# Normalizing approach 2\n",
    "scaler_2.fit(training_data_2)\n",
    "normalized_training_data_2 = scaler_2.transform(training_data_2)\n",
    "normalized_testing_data_2 = scaler_2.transform(testing_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying dimensionality reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(normalized_training_data_2)\n",
    "reduced_training_data_2 = pca.transform(normalized_training_data_2)\n",
    "reduced_testing_data_2 = pca.transform(normalized_testing_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced training data shape:  (7296, 870)\n"
     ]
    }
   ],
   "source": [
    "print(\"Reduced training data shape: \", reduced_training_data_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(data, k, max_iter=1000, tol=1e-6):\n",
    "    # Initialize centroids randomly\n",
    "    centroids = [data[i] for i in np.random.choice(range(len(data)), k, replace=False)]\n",
    "    \n",
    "    # Initialize cluster assignments\n",
    "    clusters = np.zeros(len(data))\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        # Assign each data point to the nearest centroid\n",
    "        for i, point in enumerate(data):\n",
    "            distances = np.linalg.norm(point - centroids, axis=1)\n",
    "            clusters[i] = np.argmin(distances)\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids = np.array([np.mean(data[clusters == i], axis=0) for i in range(k)])\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.linalg.norm(new_centroids - centroids) < tol:\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "    \n",
    "    return clusters, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, centroids):\n",
    "    clusters = np.zeros(len(data))\n",
    "    for i, point in enumerate(data):\n",
    "        distances = np.linalg.norm(point - centroids, axis=1)\n",
    "        clusters[i] = np.argmin(distances)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_mat(clusters, labels, k, n):\n",
    "    confusion_matrix = np.zeros((k, n))\n",
    "    for i in range(len(labels)):\n",
    "        confusion_matrix[int(clusters[i])][labels[i]] += 1\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assign_clusters_labels(clusters, labels, k, n):\n",
    "    confusion_matrix = get_confusion_mat(clusters, labels, k, n)\n",
    "    return np.argmax(confusion_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(clusters, labels, cluster_assignments):\n",
    "    correct = 0\n",
    "    for i in range(len(clusters)):\n",
    "        if cluster_assignments[int(clusters[i])] == labels[i]:\n",
    "            correct += 1\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_separator():\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans(training_data, testing_data, training_labels, testing_labels, k, n):\n",
    "    training_predictions, centroids = kmeans(training_data, k)\n",
    "    testing_predictions = predict(testing_data, centroids)\n",
    "    cluster_assignments = assign_clusters_labels(training_predictions, training_labels, k, n)\n",
    "    return get_accuracy(training_predictions, training_labels, cluster_assignments), get_accuracy(testing_predictions, testing_labels, cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_line(s1, s2):\n",
    "    return \"{:<40} {:<40}\".format(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(k, training_accuracy1, testing_accuracy1, training_accuracy2, testing_accuracy2):\n",
    "    print(f'For k = {k}')\n",
    "    print(format_line('approach #1', 'approach #2'))\n",
    "    print(format_line(f'training accuracy: {training_accuracy1:.5f}', f'training accuracy: {training_accuracy2:.5f}'))\n",
    "    print(format_line(f'testing accuracy: {testing_accuracy1:.5f}', f'testing accuracy: {testing_accuracy2:.5f}'))\n",
    "    print_separator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k = 8\n",
      "approach #1                              approach #2                             \n",
      "training accuracy: 0.34060               training accuracy: 0.36760              \n",
      "testing accuracy: 0.33936                testing accuracy: 0.36458               \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "For k = 13\n",
      "approach #1                              approach #2                             \n",
      "training accuracy: 0.45696               training accuracy: 0.44463              \n",
      "testing accuracy: 0.45340                testing accuracy: 0.44901               \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "For k = 19\n",
      "approach #1                              approach #2                             \n",
      "training accuracy: 0.62774               training accuracy: 0.51014              \n",
      "testing accuracy: 0.62116                testing accuracy: 0.50384               \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "For k = 28\n",
      "approach #1                              approach #2                             \n",
      "training accuracy: 0.70628               training accuracy: 0.57209              \n",
      "testing accuracy: 0.70779                testing accuracy: 0.56798               \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "For k = 38\n",
      "approach #1                              approach #2                             \n",
      "training accuracy: 0.71299               training accuracy: 0.69627              \n",
      "testing accuracy: 0.71436                testing accuracy: 0.70066               \n",
      "--------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Ks = [8, 13, 19, 28, 38]\n",
    "for k in Ks:\n",
    "    training_accuracy1, testing_accuracy1 = run_kmeans(normalized_training_data_1, normalized_testing_data_1, training_labels_1, testing_labels_1, k, 19)\n",
    "    training_accuracy2, testing_accuracy2 = run_kmeans(reduced_training_data_2, reduced_testing_data_2, training_labels_2, testing_labels_2, k, 19)\n",
    "    print_results(k, training_accuracy1, testing_accuracy1, training_accuracy2, testing_accuracy2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
