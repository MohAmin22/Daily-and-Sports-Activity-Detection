{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import partial as partial_func\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating taining and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/mohamed/CSED_25/Year_3/term_2/PR-ML/my_labs/lab_2/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s41.txt', 's11.txt', 's04.txt', 's50.txt', 's43.txt', 's35.txt', 's20.txt', 's15.txt', 's31.txt', 's37.txt', 's49.txt', 's07.txt', 's26.txt', 's21.txt', 's44.txt', 's58.txt', 's53.txt', 's25.txt', 's48.txt', 's34.txt', 's19.txt', 's17.txt', 's30.txt', 's42.txt', 's47.txt', 's13.txt', 's51.txt', 's14.txt', 's27.txt', 's09.txt', 's28.txt', 's12.txt', 's18.txt', 's59.txt', 's56.txt', 's46.txt', 's22.txt', 's33.txt', 's08.txt', 's54.txt', 's05.txt', 's55.txt', 's10.txt', 's03.txt', 's24.txt', 's57.txt', 's29.txt', 's23.txt', 's45.txt', 's32.txt', 's60.txt', 's36.txt', 's16.txt', 's52.txt', 's01.txt', 's38.txt', 's06.txt', 's02.txt', 's40.txt', 's39.txt']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"/home/mohamed/CSED_25/Year_3/term_2/PR-ML/my_labs/lab_2/data/a03/p1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "  \"\"\"\n",
    "    Return list of all 125 rows (125 * 45)\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "      row = [float(value) for value in line.strip().split(',')] # Converting to float\n",
    "      data.append(row)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_2_generator(list_of_rows):\n",
    "    return [item for row in list_of_rows for item in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_1_generator(list_of_rows):\n",
    "    n = len(list_of_rows)\n",
    "    mean_sample = [0 for _ in range(len(list_of_rows[0]))]\n",
    "    for row in list_of_rows:\n",
    "        for i in range(len(row)):\n",
    "            mean_sample[i] += row[i]\n",
    "    return [x / n for x in mean_sample]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(approach): \n",
    "    \"\"\"\n",
    "        This function generates and splits tainging and tesing data condering the approach desired. \n",
    "        approach = 1 -> Taking the mean of each column in each segment resulting in 45 features for each data point.\n",
    "        approach = 2 -> Flattening all the features together in 45 x 125 = 5625 features for each data point.\n",
    "    \"\"\"\n",
    "    training_data, training_labels, testing_data, testing_labels = [], [], [], []\n",
    "\n",
    "    for activity in sorted(os.listdir(path)):\n",
    "        label = int(activity[1:]) - 1   # To make it zero-based\n",
    "        subjects_path = os.path.join(path, activity)\n",
    "        # subject_path = path + '/' + activity\n",
    "        for subject in sorted(os.listdir(subjects_path)):\n",
    "            segments_path = os.path.join(subjects_path, subject)\n",
    "            \n",
    "            for segment in sorted(os.listdir(segments_path)):\n",
    "                file_name =  os.path.join(segments_path, segment)\n",
    "                data_sample = []\n",
    "                \n",
    "                if approach == 1:\n",
    "                    data_sample = approach_1_generator(read_file(file_name))\n",
    "                elif approach == 2:\n",
    "                    data_sample = approach_2_generator(read_file(file_name))\n",
    "    \n",
    "                if int(segment[1:3]) <= 48: # Belongs to training data\n",
    "                    training_data.append(data_sample)\n",
    "                    training_labels.append(label)\n",
    "                else:\n",
    "                    testing_data.append(data_sample)\n",
    "                    testing_labels.append(label)\n",
    "    return  training_data , training_labels , testing_data , testing_labels\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generated by taking the mean of each column in each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1 , training_labels_1 , testing_data_1 , testing_labels_1 = generate_data(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generated by flattening all the features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_2 , training_labels_2 , testing_data_2 , testing_labels_2 = generate_data(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data in approach-1 is considered to be (7296 * 45) but it's (7296 * 45)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data in approach-1 is considered to be (7296 * 45) but it's ({len(training_data_1)} * {len(training_data_1[0])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data in approach-2 is considered to be (7296 * 5625) but it's (7296 * 5625)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data in approach-2 is considered to be (7296 * 5625) but it's ({len(training_data_2)} * {len(training_data_2[0])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.975714400000001, 1.0831504800000005, 5.6068464, 0.004897151999999998, 0.026122679999999985, -0.003726408, -0.7907260800000004, -0.06849034400000001, 0.13589679999999996, 0.67913376, 5.713088000000003, 7.926788800000002, 0.012881520000000002, -0.0010626799999999997, -0.0029510400000000007, -0.5693321600000001, -0.56161984, -0.21113576000000006, 3.4033168000000016, -8.375712799999999, 3.8520752000000007, -0.0048876879999999985, -2.1167999999999978e-05, -0.007604448000000001, -0.64785496, 0.34177119999999994, 0.07227340800000005, -3.5357359999999978, 9.0633328, -0.9345346399999999, 0.00044342399999999993, 0.007741840000000001, -0.004339024, 0.7302159200000004, -0.25215672000000006, -0.035894191999999984, -2.8148328, -9.085131199999996, 2.618207200000001, -0.0050357039999999985, 0.002166143999999999, -0.003154824000000001, 0.7396147199999998, 0.3013141599999999, -0.05711876800000003]\n"
     ]
    }
   ],
   "source": [
    "print(training_data_1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing featrues for approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(training_data_2)\n",
    "normalized_training_data_2 = scaler.transform(training_data_2)\n",
    "normalized_testing_data_2 = scaler.transform(testing_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying dimensionality reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(normalized_training_data_2)\n",
    "reduced_training_data_2 = pca.transform(normalized_training_data_2)\n",
    "reduced_testing_data_2 = pca.transform(normalized_testing_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the effect of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of reduced flattened training data : 7296 * 870\n",
      "The dimensions of reduced flattened testing data : 1824 * 870\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dimensions of reduced flattened training data : {len(reduced_training_data_2)} * {len(reduced_training_data_2[0])}\")\n",
    "print(f\"The dimensions of reduced flattened testing data : {len(reduced_testing_data_2)} * {len(reduced_testing_data_2[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectural clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF_kernel(v1, v2, gamma):\n",
    " d = np.linalg.norm(v1 - v2)\n",
    " return np.exp(-gamma * d**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(v1, v2):\n",
    "    return np.dot(v1, v2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrlar_clustering(D, k, sim_func):\n",
    " # Compute similarity matrix\n",
    "  A = np.zeros((len(D), len(D)))\n",
    "  for ind_row in range(len(D)):\n",
    "      for ind_col in range(len(D)):\n",
    "        A[ind_row][ind_col] = sim_func(D[ind_row], D[ind_col])\n",
    "        \n",
    "  # Compute degree matrix\n",
    "  degrees = np.sum(A, axis=1)\n",
    "  degree_mat = np.diag(degrees)\n",
    "\n",
    "  # Compute Laplacian asymetric matrix\n",
    "  la = np.eye(len(D)) - np.linalg.inv(degree_mat) @ A\n",
    "\n",
    "  eigenvalues, eigenvectors = np.linalg.eig(la)\n",
    "\n",
    "  # Sort eigenvalues and eigenvectors in ascending order\n",
    "  sorted_indices = np.argsort(eigenvalues)\n",
    "  sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "  sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "  # Choose the smallest K eigenvectors\n",
    "  smallest_eigenvectors = sorted_eigenvectors[:k, :]\n",
    "  \n",
    "  # Normalize each row\n",
    "  reduced_data  = (np.real(smallest_eigenvectors).T)\n",
    "  for i in range(len(reduced_data)):\n",
    "      reduced_data[i] = (reduced_data[i] / np.linalg.norm(reduced_data[i]))\n",
    "\n",
    "  return reduced_data\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Normalized Cut to app. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(cluster_ids, labels):\n",
    "    \"\"\"\n",
    "        Computes confusion matrix given cluster ids (zero-based), labels (zero-based).\n",
    "        returns confusion matrix of size: |unique cluster_ids| * |unique labels|\n",
    "    \"\"\"    \n",
    "    assert len(cluster_ids) == len(labels)\n",
    "    n = max(cluster_ids) + 1\n",
    "    m = max(labels) + 1\n",
    "    confusion_mat = np.zeros((n, m))\n",
    "    for cluster, label in zip(cluster_ids, labels):\n",
    "        confusion_mat[cluster][label] += 1\n",
    "    return confusion_mat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_cluster_label(confusion_mat):\n",
    "    \"\"\"The index of the list is the cluster id, whereas list element is the corresponding label\"\"\"\n",
    "    return np.argmax(confusion_mat, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma_list = [1] # [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "# k = 19\n",
    "\n",
    "# for gamma in gamma_list:\n",
    "#     RBF_kernel_gamma = partial_func(RBF_kernel, gamma=gamma)\n",
    "#     reduced_training_data_spectral_RBF = spectrlar_clustering(reduced_training_data_2, k, RBF_kernel_gamma)\n",
    "\n",
    "#     # Cluster traing data using k-means and return cluster id for each training sample\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(reduced_training_data_spectral_RBF)\n",
    "#     cluster_ids = kmeans.labels_\n",
    "    \n",
    "#     # Create confusion matrix for matching training data clusters\n",
    "#     confusion_mat = confusion_matrix(cluster_ids, training_labels_2)\n",
    "    \n",
    "    \n",
    "#     # # Match the clusters with labels and return the mapping : cluster_id -> label\n",
    "#     # cluster_label_mapping_list = match_cluster_label(confusion_mat)\n",
    "    \n",
    "#     # # Pridect testing data clusters\n",
    "#     # kmeans.predict(reduced_testing_data_2)\n",
    "#     # # Transform cluster_id into label\n",
    "#     # # Compute accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Normalized Cut to app. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_list = [1] # [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "k = 19\n",
    "\n",
    "# for gamma in gamma_list:\n",
    "gamma = 1\n",
    "RBF_kernel_gamma = partial_func(RBF_kernel, gamma=gamma)\n",
    "\n",
    "reduced_training_data_spectral_RBF = spectrlar_clustering(reduced_training_data_2, k, RBF_kernel_gamma)\n",
    "\n",
    "# Cluster traing data using k-means and return cluster id for each training sample\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(reduced_training_data_spectral_RBF)\n",
    "cluster_ids = kmeans.labels_\n",
    "\n",
    "# Create confusion matrix for matching training data clusters\n",
    "confusion_mat = confusion_matrix(cluster_ids, training_labels_2)\n",
    "perform_external_measures(confusion_mat, len(reduced_training_data_spectral_RBF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLustering evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(confusion_mat, number_of_samples):\n",
    "    return sum(np.max(confusion_mat, axis=1)) / number_of_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recall(confusion_mat, number_of_samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_measure(confusion_mat):\n",
    "    f_score = 0\n",
    "    col_sum_list = np.sum(confusion_mat, axis=0)\n",
    "    for row in confusion_mat:\n",
    "        max_element = np.max(row)\n",
    "        max_ind = np.argmax(row)\n",
    "        purity = (max_element / np.sum(row))\n",
    "        recall = (max_element / col_sum_list[max_ind])\n",
    "        f_score += 2 * ((purity * recall) / (purity + recall))\n",
    "    return f_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(confusion_mat, number_of_samples):\n",
    "    entropy = 0\n",
    "    for row in confusion_mat:\n",
    "        num_elements_of_cluster = np.sum(row)\n",
    "        cluster_entropy = 0\n",
    "        for col in row:\n",
    "            p = col / num_elements_of_cluster\n",
    "            cluster_entropy -=  (p * np.log2(p))\n",
    "        entropy += ((num_elements_of_cluster / number_of_samples) * cluster_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_external_measures(confusion_mat, number_of_samples):\n",
    "    print(f\"Precision is {precision(confusion_mat, number_of_samples)}\")\n",
    "    print(f\"F-score is {f_measure(confusion_mat)}\")\n",
    "    print(f\"Conditional entropy is {conditional_entropy(confusion_mat, number_of_samples)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
