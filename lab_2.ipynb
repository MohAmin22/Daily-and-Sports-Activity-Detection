{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import partial as partial_func\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:rgb(100, 149, 237)'> Data preprocessing<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating taining and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/mohamed/CSED_25/Year_3/term_2/PR-ML/my_labs/lab_2/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s41.txt', 's11.txt', 's04.txt', 's50.txt', 's43.txt', 's35.txt', 's20.txt', 's15.txt', 's31.txt', 's37.txt', 's49.txt', 's07.txt', 's26.txt', 's21.txt', 's44.txt', 's58.txt', 's53.txt', 's25.txt', 's48.txt', 's34.txt', 's19.txt', 's17.txt', 's30.txt', 's42.txt', 's47.txt', 's13.txt', 's51.txt', 's14.txt', 's27.txt', 's09.txt', 's28.txt', 's12.txt', 's18.txt', 's59.txt', 's56.txt', 's46.txt', 's22.txt', 's33.txt', 's08.txt', 's54.txt', 's05.txt', 's55.txt', 's10.txt', 's03.txt', 's24.txt', 's57.txt', 's29.txt', 's23.txt', 's45.txt', 's32.txt', 's60.txt', 's36.txt', 's16.txt', 's52.txt', 's01.txt', 's38.txt', 's06.txt', 's02.txt', 's40.txt', 's39.txt']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"/home/mohamed/CSED_25/Year_3/term_2/PR-ML/my_labs/lab_2/data/a03/p1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "  \"\"\"\n",
    "    Return list of all 125 rows (125 * 45)\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "      row = [float(value) for value in line.strip().split(',')] # Converting to float\n",
    "      data.append(row)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_2_generator(list_of_rows):\n",
    "    return [item for row in list_of_rows for item in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_1_generator(list_of_rows):\n",
    "    n = len(list_of_rows)\n",
    "    mean_sample = [0 for _ in range(len(list_of_rows[0]))]\n",
    "    for row in list_of_rows:\n",
    "        for i in range(len(row)):\n",
    "            mean_sample[i] += row[i]\n",
    "    return [x / n for x in mean_sample]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(approach): \n",
    "    \"\"\"\n",
    "        This function generates and splits tainging and tesing data condering the approach desired. \n",
    "        approach = 1 -> Taking the mean of each column in each segment resulting in 45 features for each data point.\n",
    "        approach = 2 -> Flattening all the features together in 45 x 125 = 5625 features for each data point.\n",
    "    \"\"\"\n",
    "    training_data, training_labels, testing_data, testing_labels = [], [], [], []\n",
    "\n",
    "    for activity in sorted(os.listdir(path)):\n",
    "        label = int(activity[1:]) - 1   # To make it zero-based\n",
    "        subjects_path = os.path.join(path, activity)\n",
    "        # subject_path = path + '/' + activity\n",
    "        for subject in sorted(os.listdir(subjects_path)):\n",
    "            segments_path = os.path.join(subjects_path, subject)\n",
    "            \n",
    "            for segment in sorted(os.listdir(segments_path)):\n",
    "                file_name =  os.path.join(segments_path, segment)\n",
    "                data_sample = []\n",
    "                \n",
    "                if approach == 1:\n",
    "                    data_sample = approach_1_generator(read_file(file_name))\n",
    "                elif approach == 2:\n",
    "                    data_sample = approach_2_generator(read_file(file_name))\n",
    "    \n",
    "                if int(segment[1:3]) <= 48: # Belongs to training data\n",
    "                    training_data.append(data_sample)\n",
    "                    training_labels.append(label)\n",
    "                else:\n",
    "                    testing_data.append(data_sample)\n",
    "                    testing_labels.append(label)\n",
    "    return  training_data , training_labels , testing_data , testing_labels\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generated by taking the mean of each column in each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1 , training_labels_1 , testing_data_1 , testing_labels_1 = generate_data(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generated by flattening all the features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_2 , training_labels_2 , testing_data_2 , testing_labels_2 = generate_data(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data in approach-1 is considered to be (7296 * 45) but it's (7296 * 45)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data in approach-1 is considered to be (7296 * 45) but it's ({len(training_data_1)} * {len(training_data_1[0])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data in approach-2 is considered to be (7296 * 5625) but it's (7296 * 5625)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data in approach-2 is considered to be (7296 * 5625) but it's ({len(training_data_2)} * {len(training_data_2[0])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing featrues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_1, scaler_2 = MinMaxScaler(), MinMaxScaler()\n",
    "# Normalizing approach 1\n",
    "# TODO: remove\n",
    "scaler_1.fit(training_data_1)\n",
    "normalized_training_data_1 = scaler_1.transform(training_data_1)\n",
    "normalized_testing_data_1 = scaler_1.transform(testing_data_1)\n",
    "\n",
    "# Normalizing approach 2\n",
    "scaler_2.fit(training_data_2)\n",
    "normalized_training_data_2 = scaler_2.transform(training_data_2)\n",
    "normalized_testing_data_2 = scaler_2.transform(testing_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying dimensionality reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=0.95)\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(normalized_training_data_2)\n",
    "reduced_training_data_2 = pca.transform(normalized_training_data_2)\n",
    "reduced_testing_data_2 = pca.transform(normalized_testing_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the effect of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of reduced flattened training data : 7296 * 299\n",
      "The dimensions of reduced flattened testing data : 1824 * 299\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dimensions of reduced flattened training data : {len(reduced_training_data_2)} * {len(reduced_training_data_2[0])}\")\n",
    "print(f\"The dimensions of reduced flattened testing data : {len(reduced_testing_data_2)} * {len(reduced_testing_data_2[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:rgb(100, 149, 237)'> Clustering evaluation<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contingency_table(cluster_ids, labels):\n",
    "    \"\"\"\n",
    "        Computes contingency_table given cluster ids (zero-based), labels (zero-based).\n",
    "        returns contingency_table of size: |unique cluster_ids| * |unique labels|\n",
    "    \"\"\"    \n",
    "    assert len(cluster_ids) == len(labels)\n",
    "    n = max(cluster_ids) + 1\n",
    "    m = max(labels) + 1\n",
    "    contingency_table = np.zeros((n, m))\n",
    "    for cluster, label in zip(cluster_ids, labels):\n",
    "        contingency_table[cluster][label] += 1\n",
    "    return contingency_table    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_cluster_label(contingency_table):\n",
    "    \"\"\"The index of the list is the cluster id, whereas list element is the corresponding label\"\"\"\n",
    "    return np.argmax(contingency_table, axis = 1) # 0-based index labels if you want to match the real label add plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(contingency_table):\n",
    "    tp = 0\n",
    "    for row in contingency_table:\n",
    "        for elem in row:\n",
    "            tp += (elem * (elem - 1) / 2)\n",
    "    return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positive(contingency_table):\n",
    "    fp = 0\n",
    "    for row in contingency_table:\n",
    "        for i in range(len(row)):\n",
    "            for j in range(i + 1, len(row)):\n",
    "                fp += (row[i] * row [j])\n",
    "    return fp            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(contingency_table):\n",
    "    \"\"\"\n",
    "        Calculates confusion matrix from contingency table\n",
    "        return: True positive, True negative, False positive, False negative\n",
    "    \"\"\"\n",
    "    tp, tn, fp, fn = 0 ,0 ,0 ,0\n",
    "    # True positive\n",
    "    tp  = true_positive(contingency_table)\n",
    "    \n",
    "    # False positive \n",
    "    fp = false_positive(contingency_table)\n",
    "                \n",
    "    # False negative\n",
    "    fn = false_positive(contingency_table.T)\n",
    "    \n",
    "     \n",
    "    # True negative\n",
    "    tn = np.sum(contingency_table) - (tp + fp + fn)\n",
    "    \n",
    "    return tp, tn, fp, fn\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(contingency_table, number_of_samples):\n",
    "    return sum(np.max(contingency_table, axis=1)) / number_of_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_confusion(tp, tn, fp, fn):\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp, tn, fp, fn):\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_measure(contingency_table):\n",
    "    f_score = 0\n",
    "    col_sum_list = np.sum(contingency_table, axis=0)\n",
    "    for row in contingency_table:\n",
    "        max_element = np.max(row)\n",
    "        max_ind = np.argmax(row)\n",
    "        purity = (max_element / np.sum(row))\n",
    "        recall = (max_element / col_sum_list[max_ind])\n",
    "        f_score += 2 * ((purity * recall) / (purity + recall))\n",
    "    return f_score / len(contingency_table)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(contingency_table, number_of_samples):\n",
    "    entropy = 0\n",
    "    for row in contingency_table:\n",
    "        num_elements_of_cluster = np.sum(row)\n",
    "        cluster_entropy = 0\n",
    "        for col in row:\n",
    "            p = col / num_elements_of_cluster\n",
    "            if p != 0:\n",
    "                cluster_entropy -=  (p * np.log2(p))\n",
    "        entropy += ((num_elements_of_cluster / number_of_samples) * cluster_entropy)\n",
    "    return entropy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_external_measures(contingency_table, number_of_samples):\n",
    "    tp, tn, fp, fn = confusion_matrix(contingency_table)\n",
    "    print(f\"Precision is {precision(contingency_table, number_of_samples)}\")\n",
    "    print(f\"Precision confusion is {precision_confusion(tp, tn, fp, fn)}\")\n",
    "    print(f\"Recall is {recall(tp, tn, fp, fn)}\")\n",
    "    print(f\"F-score is {f_measure(contingency_table)}\")\n",
    "    print(f\"Conditional entropy is {conditional_entropy(contingency_table, number_of_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [8, 13, 19, 28, 38]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:purple\">Spectral clustering</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF_kernel(v1, v2, gamma):\n",
    "    d = np.linalg.norm(v1 - v2)\n",
    "    return np.exp(-gamma * d**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_similarity_k(k):\n",
    "    def knn_similarity(D):\n",
    "        model = NearestNeighbors(n_neighbors=k, metric='euclidean')\n",
    "        model.fit(D)\n",
    "\n",
    "        # Compute k-NN indices\n",
    "        distances, indices = model.kneighbors(D)\n",
    "        \n",
    "        n = len(D)\n",
    "        # Create an empty adjacency matrix\n",
    "        A = np.zeros((n, n))\n",
    "\n",
    "        # Fill in the similarity matrix\n",
    "        for i in range(n):\n",
    "            for j in indices[i]:\n",
    "                A[i, j] = 1\n",
    "                A[j, i] = 1  # For symmetry: j is also a neighbor of i\n",
    "\n",
    "        return A\n",
    "    return knn_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_clustering(D, k, sim_func):\n",
    "  epsilon = 1e-10\n",
    "  # Compute similarity matrix\n",
    "  A = sim_func(D)\n",
    "\n",
    "  # Compute degree matrix\n",
    "  degrees = np.sum(A, axis=1)\n",
    "  degree_mat = np.diag(degrees)\n",
    "\n",
    "  # Compute Laplacian asymetric matrix\n",
    "  la = np.eye(len(D)) - np.linalg.inv(degree_mat) @ A\n",
    "\n",
    "  eigenvalues, eigenvectors = np.linalg.eig(la)\n",
    "\n",
    "  # Sort eigenvalues and eigenvectors in ascending order\n",
    "  sorted_indices = np.argsort(eigenvalues)\n",
    "  sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "  sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "  # Choose the smallest K eigenvectors\n",
    "  smallest_eigenvectors = sorted_eigenvectors[:k, :]\n",
    "  \n",
    "  # Normalize each row\n",
    "  reduced_data  = (np.real(smallest_eigenvectors).T)\n",
    "  \n",
    "  for i in range(len(reduced_data)):\n",
    "        reduced_data[i] = reduced_data[i] / (np.linalg.norm(reduced_data[i]) + epsilon)\n",
    "  return reduced_data\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State init\n",
    "gamma_list = [0.0001] # Don't ever use gamma = 1\n",
    "k = 19\n",
    "# Function init\n",
    "knn_sim = knn_similarity_k(20)\n",
    "sim_func_list = [knn_sim, cosine_similarity, np.corrcoef]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:white\">Applying spectral clustering to app. 1</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of spectral clustering using similarity measure : knn_similarity\n",
      "Precision is 0.05646929824561404\n",
      "Precision confusion is 0.052501777261782824\n",
      "Recall is 0.9893392709907929\n",
      "F-score is 0.012920064489737421\n",
      "Conditional entropy is 4.2288508697757194\n",
      "Evaluation of spectral clustering using similarity measure : cosine_similarity\n",
      "Precision is 0.4041940789473684\n",
      "Precision confusion is 0.27964679575782936\n",
      "Recall is 0.3314567014795474\n",
      "F-score is 0.40658191423190837\n",
      "Conditional entropy is 2.127430300505754\n",
      "Evaluation of spectral clustering using similarity measure : corrcoef\n",
      "Precision is 0.40803179824561403\n",
      "Precision confusion is 0.2605719377591839\n",
      "Recall is 0.31017460835509136\n",
      "F-score is 0.41287654117202466\n",
      "Conditional entropy is 2.1739462543140062\n"
     ]
    }
   ],
   "source": [
    "number_of_samples_spectral_app_1 = len(normalized_training_data_1)\n",
    "for sim_func in sim_func_list:\n",
    "    # RBF_kernel_gamma = partial_func(RBF_kernel, gamma=gamma)\n",
    "    print(f\"Evaluation of spectral clustering using similarity measure : {sim_func.__name__}\")\n",
    "    reduced_training_data_spectral_cosine_app_1 = spectral_clustering(normalized_training_data_1, k, sim_func) # Replace test\n",
    "\n",
    "    # Cluster traing data using k-means and return cluster id for each training sample\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(reduced_training_data_spectral_cosine_app_1)\n",
    "    cluster_ids_spectral_app_1 = kmeans.labels_\n",
    "    \n",
    "    # Create contingency table matrix for matching training data clusters\n",
    "    contingency_table_spectral_app_1 = contingency_table(cluster_ids_spectral_app_1, training_labels_1)\n",
    "    \n",
    "    # Evaluate clustering\n",
    "    perform_external_measures(contingency_table_spectral_app_1, number_of_samples_spectral_app_1)\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying spectral clustering to app. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is 0.44051535087719296\n",
      "Precision confusion is 0.25394631765858255\n",
      "Recall is 0.37166901424579724\n",
      "F-score is 0.46152189650248354\n",
      "Conditional entropy is 1.8807243670517555\n"
     ]
    }
   ],
   "source": [
    "gamma_list = [0.0001] # Don't ever use gamma = 1\n",
    "k = 19\n",
    "number_of_samples_spectral_app_2 = len(reduced_training_data_2)\n",
    "for gamma in gamma_list:\n",
    "    # RBF_kernel_gamma = partial_func(RBF_kernel, gamma=gamma)\n",
    "    reduced_training_data_spectral_cosine_app_2 = spectral_clustering(reduced_training_data_2, k, cosine_sim) # Replace test\n",
    "\n",
    "    # Cluster traing data using k-means and return cluster id for each training sample\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(reduced_training_data_spectral_cosine_app_2)\n",
    "    cluster_ids_spectral_app_2 = kmeans.labels_\n",
    "    \n",
    "    # Create contingency table for matching training data clusters\n",
    "    contingency_table_spectral_app_2 = contingency_table(cluster_ids_spectral_app_2, training_labels_2)\n",
    "    \n",
    "    # Evaluate clustering\n",
    "    perform_external_measures(contingency_table_spectral_app_2, number_of_samples_spectral_app_2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
